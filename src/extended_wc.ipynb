{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibWjDQ3UdCeH",
        "outputId": "b685c393-2d20-468f-ec55-046c80b7a5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gom76v0dccK",
        "outputId": "75bd8f0d-527d-4435-c94a-9a862cc3b927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Puz-197LdcYo",
        "outputId": "dea81e53-73fa-44f7-bbda-55186824b4ed"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "(async (port, path, text, element) => {\n    if (!google.colab.kernel.accessAllowed) {\n      return;\n    }\n    element.appendChild(document.createTextNode(''));\n    const url = await google.colab.kernel.proxyPort(port);\n    const anchor = document.createElement('a');\n    anchor.href = new URL(path, url).toString();\n    anchor.target = '_blank';\n    anchor.setAttribute('data-href', url + path);\n    anchor.textContent = text;\n    element.appendChild(anchor);\n  })(4040, \"/jobs/index.html\", \"https://localhost:4040/jobs/index.html\", window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(4040, path='/jobs/index.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLHl__UOdF0a",
        "outputId": "f20a000e-bd87-4150-9789-669b34e2ceb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"Word_Count_App\")\n",
        "\n",
        "\n",
        "def get_tokens_list(data):\n",
        "\tif data.strip():\n",
        "\t\tdata = data.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "\treturn [token.lower() for token in data.split()]\n",
        "\n",
        "\n",
        "def word_frequency(combined_data):\n",
        "\ttokens = combined_data.flatMap(lambda data: get_tokens_list(data))\n",
        "\n",
        "\tstop_words = set(stopwords.words(\"english\"))\n",
        "\ttokens_without_stopwords = tokens.filter(lambda token: token not in stop_words)\n",
        "\n",
        "\ttokens_count = tokens_without_stopwords.map(lambda token: (token, 1))\n",
        "\tfinal_tokens_count = tokens_count.reduceByKey(lambda i, j: i + j)\n",
        "\n",
        "\tsorted_tokens_count = final_tokens_count.sortBy(lambda items: items[1], ascending=False)\n",
        "\tsorted_tokens_count = sorted_tokens_count.filter(lambda x: x[0] != '')\n",
        "\n",
        "\treturn sorted_tokens_count\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "\tpath_1 = \"/content/drive/MyDrive/Colab Notebooks/dataset/DRACULA.txt\"\n",
        "\tpath_2 = \"/content/drive/MyDrive/Colab Notebooks/dataset/Metamorphosis.txt\"\n",
        "\toutput_path = \"/content/drive/MyDrive/Colab Notebooks/dataset/extended_wc_output/\"\n",
        "\tfile1_data = sc.textFile(path_1)\n",
        "\tfile2_data = sc.textFile(path_2)\n",
        "\tcombined_data = file1_data.union(file2_data)\n",
        "\n",
        "\tsorted_word_count_frequency = word_frequency(combined_data)\n",
        "\tsorted_word_count_frequency.coalesce(1).saveAsTextFile(output_path)\n",
        "\ttime.sleep(180)\n",
        "\tsc.stop()\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8t4Qw1MdFfw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
